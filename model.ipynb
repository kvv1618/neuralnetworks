{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDcZnwxA70sV",
        "outputId": "363d1758-e4a3-49bd-bee3-8f86ddc98ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive' , force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "wJ0FNvRsdpSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68bf8906-75ef-42f2-ab56-ef0f843fadb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ZBe25j9kvv"
      },
      "source": [
        "### Preprocess Climate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GM6HlYq9D_B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/NNProject/cleaned_climate_pca_extensive.csv'\n",
        "\n",
        "climate_df = pd.read_csv(file_path)\n",
        "climate_df['date'] = climate_df['date'].str.slice(0, 10)\n",
        "climate_df['date'] = pd.to_datetime(climate_df['date'], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
        "climate_df.index = climate_df['date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B13nFXgeBw8J"
      },
      "outputs": [],
      "source": [
        "climate_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6xgiDvl9Xzg"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "numeric_cols = climate_df.select_dtypes(include=['float64', 'int64']).columns.to_list()\n",
        "numeric_cols.remove('lat')\n",
        "numeric_cols.remove('lon')\n",
        "print(numeric_cols)\n",
        "non_numeric_cols = climate_df.select_dtypes(exclude=['float64', 'int64']).columns.to_list() + ['lat', 'lon']\n",
        "print(non_numeric_cols)\n",
        "climate_df[numeric_cols] = climate_df[numeric_cols].astype(float)\n",
        "climate_df[non_numeric_cols] = climate_df[non_numeric_cols]\n",
        "scaler = MinMaxScaler()\n",
        "scaled_df = pd.DataFrame(scaler.fit_transform(climate_df[numeric_cols]), columns=numeric_cols, index=climate_df.index)\n",
        "\n",
        "climate_data = pd.concat([scaled_df, climate_df[non_numeric_cols]], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UURolnR0BrKS"
      },
      "outputs": [],
      "source": [
        "climate_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag-kXvFbxJJU"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(climate_data[\"lat\"].unique())\n",
        "print(climate_data[\"lon\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZJ0sZAq9ogi"
      },
      "source": [
        "### Preprocess Energy Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR1D62-Q9aFv"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/NNProject/daily_energy.csv'\n",
        "\n",
        "energy_df = pd.read_csv(file_path)\n",
        "energy_df['Dates'] = energy_df['Dates'].str.slice(0, 10)\n",
        "energy_df['Dates'] = pd.to_datetime(energy_df['Dates'], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
        "energy_df.index = energy_df['Dates']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3ng10uf9-W2"
      },
      "outputs": [],
      "source": [
        "\n",
        "numeric_cols = energy_df.select_dtypes(include=['float64', 'int64']).columns.to_list()\n",
        "numeric_cols.remove('latitude')\n",
        "numeric_cols.remove('longitude')\n",
        "non_numeric_cols = energy_df.select_dtypes(exclude=['float64', 'int64']).columns.to_list() + ['latitude', 'longitude']\n",
        "energy_df[numeric_cols] = energy_df[numeric_cols].astype(float)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_df = pd.DataFrame(scaler.fit_transform(energy_df[numeric_cols]), columns=numeric_cols, index=energy_df.index)\n",
        "energy_data = pd.concat([scaled_df, energy_df[non_numeric_cols]], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-QsWQtI-BOT"
      },
      "source": [
        "### State-Lat-Lon Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NBhs-LM9_xa"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "lat_range = np.array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32])\n",
        "lon_range = np.array([72, 74, 76, 78, 80, 82, 84, 86])\n",
        "\n",
        "energy_data[\"latitude\"] = energy_data[\"latitude\"].apply(lambda x: lat_range[np.argmin(np.abs(lat_range - x))])\n",
        "energy_data[\"longitude\"] = energy_data[\"longitude\"].apply(lambda x: lon_range[np.argmin(np.abs(lon_range - x))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySpng8nb-Jwx"
      },
      "source": [
        "## PreTrain on Climate Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtWOdSrW-MjK"
      },
      "source": [
        "### Time-Series Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avktSMlx-ElW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def lagged_climate_input(data, seq_length=28):\n",
        "    X = []\n",
        "    for _, group in data.groupby([\"lat\", \"lon\"]):\n",
        "        group = group.sort_index()\n",
        "        features = group.drop(columns=[\"date\", \"lat\", \"lon\"])\n",
        "        for row in range(len(features) - seq_length):\n",
        "            X.append(features.iloc[row:row+seq_length].values)\n",
        "    return np.array(X)\n",
        "\n",
        "climate_X = lagged_climate_input(climate_data, 12)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRjb9CQ_l4Zx"
      },
      "outputs": [],
      "source": [
        "climate_X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQsZGVDR-RuL"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpBYT8lv-OeR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ClimatePreTrainer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, latent_size=32, num_layers=1, dropout=0.3, aug_num_segments = 2):\n",
        "        super(ClimatePreTrainer, self).__init__()\n",
        "\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
        "                               batch_first=True, bidirectional=True, dropout=dropout)\n",
        "\n",
        "        self.encoder_bn = nn.BatchNorm1d(hidden_size * 2)\n",
        "\n",
        "        self.to_latent = nn.Linear(hidden_size * 2, latent_size)\n",
        "        self.from_latent = nn.Linear(latent_size, hidden_size * 2)\n",
        "\n",
        "        self.decoder = nn.LSTM(hidden_size * 2, hidden_size, num_layers=num_layers,\n",
        "                               batch_first=True, bidirectional=True, dropout=dropout)\n",
        "\n",
        "        self.decoder_bn = nn.BatchNorm1d(hidden_size * 2)\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_size * 2, input_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.aug_num_segments = aug_num_segments\n",
        "\n",
        "    def forward(self, x, augment = False):\n",
        "        if augment:\n",
        "            x = self.augment_data(x)\n",
        "\n",
        "        encoded_x, _ = self.encoder(x)\n",
        "        encoded_x = self.dropout(encoded_x)\n",
        "\n",
        "        batch_size, seq_len, feat_dim = encoded_x.shape\n",
        "        norm_encoded = self.encoder_bn(encoded_x.contiguous().view(-1, feat_dim))\n",
        "        norm_encoded = norm_encoded.view(batch_size, seq_len, feat_dim)\n",
        "\n",
        "        latent = self.to_latent(encoded_x)\n",
        "        expanded = self.from_latent(latent)\n",
        "\n",
        "        decoded_x, _ = self.decoder(expanded)\n",
        "        decoded_x = self.dropout(decoded_x)\n",
        "\n",
        "        batch_size, seq_len, feat_dim = decoded_x.shape\n",
        "        norm_decoded = self.decoder_bn(decoded_x.contiguous().view(-1, feat_dim))\n",
        "        norm_decoded = norm_decoded.view(batch_size, seq_len, feat_dim)\n",
        "\n",
        "        reconstructed_x = self.output_layer(decoded_x)\n",
        "        return reconstructed_x, latent\n",
        "\n",
        "    def augment_data(self, x):\n",
        "        \"\"\"Applies data augmentation techniques to the input data.\"\"\"\n",
        "        # 1. Jittering: Add random noise to the input\n",
        "        noise_factor = 0.05  # Adjust the noise level as needed\n",
        "        noise = torch.randn_like(x) * noise_factor\n",
        "        x = x + noise\n",
        "\n",
        "        # 2. Scaling: Scale the input by a random factor\n",
        "        scale_factor = np.random.uniform(0.9, 1.1)  # Adjust the scaling range as needed\n",
        "        x = x * scale_factor\n",
        "\n",
        "        # 3. Permutation: Randomly permute segments within the time series\n",
        "        # (You might need to adapt this based on your specific data structure)\n",
        "        # Adjust the number of segments as needed\n",
        "        segment_length = x.shape[1] // self.aug_num_segments\n",
        "        indices = torch.randperm(self.aug_num_segments)\n",
        "        permuted_x = torch.cat([x[:, i * segment_length:(i + 1) * segment_length] for i in indices], dim=1)\n",
        "        x = permuted_x\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_EQnH0t-Ue5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchinfo import summary\n",
        "\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "lr = 0.01\n",
        "\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "climate_X_tensor = torch.tensor(climate_X, dtype=torch.float32)\n",
        "\n",
        "train_X, val_X = train_test_split(climate_X_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = TensorDataset(train_X)\n",
        "val_dataset = TensorDataset(val_X)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "climate_pretrained_model = ClimatePreTrainer(input_size=climate_X.shape[2], num_layers=2, aug_num_segments=4).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(climate_pretrained_model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    climate_pretrained_model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch[0].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        reconstructed_x, _ = climate_pretrained_model(batch, augment=True)\n",
        "        loss = criterion(reconstructed_x, batch)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    climate_pretrained_model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch[0].to(device)\n",
        "            reconstructed_x, _ = climate_pretrained_model(batch)\n",
        "            loss = criterion(reconstructed_x, batch)\n",
        "            val_loss += loss.item()\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "        torch.save(climate_pretrained_model.state_dict(), 'climate_pretrained_model.pth')\n",
        "        print(f\"Validation loss improved. Saving model.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"Validation loss did not improve. Patience Counter: {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c99xOTMV-X4s"
      },
      "source": [
        "#### Freeze Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfZYqwOz-ZHH"
      },
      "outputs": [],
      "source": [
        "climate_pretrained_model = ClimatePreTrainer(input_size=climate_X.shape[2], num_layers=2).to(device)\n",
        "climate_pretrained_model.load_state_dict(torch.load('climate_pretrained_model.pth'))\n",
        "\n",
        "for param in climate_pretrained_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcasE6Y3-cLj"
      },
      "source": [
        "## FineTune Downstream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TetQV1XiBXMf"
      },
      "outputs": [],
      "source": [
        "climate_data = climate_data.reset_index(drop=True)\n",
        "energy_data = energy_data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL6KBid4-eUY"
      },
      "source": [
        "### Merge DataSources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5EkkPgZoVoR"
      },
      "outputs": [],
      "source": [
        "energy_data = energy_data.rename(columns={\"latitude\": \"lat\", \"longitude\": \"lon\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke45amvy-hwS"
      },
      "outputs": [],
      "source": [
        "merged_df = pd.merge(\n",
        "    energy_data,\n",
        "    climate_data,\n",
        "    how=\"left\",\n",
        "    left_on=[\"lat\", \"lon\", \"Dates\"],\n",
        "    right_on=[\"lat\", \"lon\", \"date\"],\n",
        "    suffixes=(\"\", \"_climate\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwx6BvSaw4g2"
      },
      "outputs": [],
      "source": [
        "merged_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzkqFU0PqniC"
      },
      "outputs": [],
      "source": [
        "\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdGFeDWkwsJH"
      },
      "outputs": [],
      "source": [
        "merged_df = merged_df.drop(columns=[\"Dates\"])\n",
        "merged_df = merged_df.dropna().reset_index(drop=True)\n",
        "merged_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_qlARC-qyM7"
      },
      "outputs": [],
      "source": [
        "merged_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wdvMEoz-jZ2"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "categorical_cols = merged_df.select_dtypes(include=[\"object\"]).columns\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    merged_df[col] = le.fit_transform(merged_df[col])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNJkK5Eq-kpL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def lagged_merged_data(data, seq_length=28):\n",
        "    X = []\n",
        "    y = []\n",
        "    for _, group in data.groupby([\"lat\", \"lon\"]):\n",
        "        group = group.sort_values(\"date\")\n",
        "        features = group.drop(columns=[\"date\", \"lat\", \"lon\", \"States\", \"Usage\"])\n",
        "        target = group[\"Usage\"]\n",
        "        for row in range(len(features) - seq_length):\n",
        "            X.append(features.iloc[row:row+seq_length].values)\n",
        "            y.append(target.iloc[row+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "merged_X, targt_y = lagged_merged_data(merged_df, 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJtQTLTjDmdv"
      },
      "outputs": [],
      "source": [
        "\n",
        "merged_X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_WFj4NT-lv1"
      },
      "outputs": [],
      "source": [
        "\n",
        "class EnergyPrediction(nn.Module):\n",
        "    def __init__(self, encoder, input_size, hidden_size=64, aug_num_segments=4):\n",
        "        super(EnergyPrediction, self).__init__()\n",
        "        self.encoder = encoder.encoder\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "        self.aug_num_segments = aug_num_segments\n",
        "\n",
        "    def forward(self, x, augment=False):\n",
        "        if augment:\n",
        "            x = self.augment_data(x)\n",
        "        encoded_x, _ = self.encoder(x)  # [batch, seq_len, hidden*2]\n",
        "        x = self.regressor(encoded_x[:, -1, :])  # Use last timestep\n",
        "        return x  # [batch, 1]\n",
        "\n",
        "    def augment_data(self, x):\n",
        "        \"\"\"Applies data augmentation techniques to the input data.\"\"\"\n",
        "        # 1. Jittering: Add random noise to the input\n",
        "        noise_factor = 0.05  # Adjust the noise level as needed\n",
        "        noise = torch.randn_like(x) * noise_factor\n",
        "        x = x + noise\n",
        "\n",
        "        # 2. Scaling: Scale the input by a random factor\n",
        "        scale_factor = np.random.uniform(0.9, 1.1)  # Adjust the scaling range as needed\n",
        "        x = x * scale_factor\n",
        "\n",
        "        # 3. Permutation: Randomly permute segments within the time series\n",
        "        # Adjust the number of segments as needed\n",
        "        segment_length = x.shape[1] // self.aug_num_segments\n",
        "        indices = torch.randperm(self.aug_num_segments)\n",
        "        permuted_x = torch.cat([x[:, i * segment_length:(i + 1) * segment_length] for i in indices], dim=1)\n",
        "        x = permuted_x\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndXMLEp9-myu"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchinfo import summary\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "merged_X_tensor = torch.tensor(merged_X, dtype=torch.float32)\n",
        "targt_y_tensor = torch.tensor(targt_y, dtype=torch.float32)\n",
        "merged_X_tensor = merged_X_tensor.view(merged_X_tensor.shape[0], merged_X_tensor.shape[1], -1)\n",
        "targt_y_tensor = targt_y_tensor.view(targt_y_tensor.shape[0], 1, -1).view(-1, 1, 1).repeat(1, merged_X_tensor.shape[1], 1)\n",
        "merged_tensor = torch.cat((merged_X_tensor, targt_y_tensor), dim=2)\n",
        "\n",
        "train, val = train_test_split(merged_tensor, test_size=0.2, random_state=42)\n",
        "train_X = train[:, :, :-1]\n",
        "train_y = train[:, :, -1]\n",
        "val_X = val[:, :, :-1]\n",
        "val_y = val[:, :, -1]\n",
        "\n",
        "train_X = train_X.view(train_X.shape[0], train_X.shape[1], -1)\n",
        "val_X = val_X.view(val_X.shape[0], val_X.shape[1], -1)\n",
        "train_loader = DataLoader(TensorDataset(train_X, train_y), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(val_X, val_y), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "climate_pretrained_model.eval()\n",
        "energy_model = EnergyPrediction(climate_pretrained_model, input_size=merged_X.shape[2], aug_num_segments=4).to(device)\n",
        "print(\"Model Summary:\")\n",
        "print(energy_model)\n",
        "print(summary(energy_model, input_size=(1, 14, merged_X.shape[2])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcwzJrdKhxWq"
      },
      "outputs": [],
      "source": [
        "lr = 0.001\n",
        "num_epochs = 50\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(energy_model.parameters(), lr=lr)\n",
        "energy_model.train()\n",
        "loss_vector = []\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        inputs, targets = batch[0].to(device), batch[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = energy_model(inputs, augment=True)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss_vector.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U68VwV29zf4r"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(loss_vector)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss With Frozen Pretrained Model')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6Xyv7rb-nza"
      },
      "outputs": [],
      "source": [
        "\n",
        "for param in energy_model.encoder.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "loss_vector = []\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        inputs, targets = batch[0].to(device), batch[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = energy_model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss_vector.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "torch.save(energy_model.state_dict(), 'energy_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K3rXzmGzf4r"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_vector)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss With Fine-tuning Model')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTKc-bggIisl"
      },
      "source": [
        "#### Model Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bboQGdLzf4r"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "energy_model.eval()\n",
        "val_predictions = []\n",
        "val_targets = []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        inputs, targets = batch[0].to(device), batch[1].to(device)\n",
        "        outputs = energy_model(inputs)\n",
        "        val_predictions.append(outputs.cpu().numpy())\n",
        "        val_targets.append(targets.cpu().numpy())\n",
        "val_predictions = np.concatenate(val_predictions)\n",
        "val_targets = np.concatenate(val_targets)\n",
        "val_predictions = scaler.inverse_transform(val_predictions)\n",
        "val_targets = scaler.inverse_transform(val_targets)\n",
        "val_targets = val_targets[:, [0]]\n",
        "mse = mean_squared_error(val_targets, val_predictions)\n",
        "mae = mean_absolute_error(val_targets, val_predictions)\n",
        "r2 = r2_score(val_targets, val_predictions)\n",
        "print(f\"Validation MSE: {mse:.4f}\")\n",
        "print(f\"Validation MAE: {mae:.4f}\")\n",
        "print(f\"Validation R2: {r2:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(val_targets, label='True Values')\n",
        "plt.plot(val_predictions, label='Predicted Values')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Energy Consumption')\n",
        "plt.title('True vs Predicted Energy Consumption')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pyx4avV-pa8"
      },
      "outputs": [],
      "source": [
        "def predict(state, date, model_path=\"energy_model.pth\", meta_path=\"energy_model.pkl\", true_data_path=\"data/daily_energy.csv\", lagged_days=14):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = EnergyPrediction(climate_pretrained_model, input_size=merged_X.shape[2]).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    true_data = pd.read_csv(true_data_path)\n",
        "    true_data = true_data[true_data['States'] == state]\n",
        "    true_data['Dates'] = true_data['Dates'].str.slice(0, 10)\n",
        "    true_data['Dates'] = pd.to_datetime(true_data['Dates'], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
        "    true_data = true_data.sort_values(\"Dates\")\n",
        "\n",
        "    end_idx = true_data[true_data['Dates'] == date].index[0]\n",
        "    start_idx = end_idx - lagged_days + 1\n",
        "    if start_idx < 0:\n",
        "        raise ValueError(\"Not enough data to predict.\")\n",
        "    past_window = true_data.iloc[start_idx:end_idx + 1]\n",
        "    features = past_window.drop(columns=[\"Dates\", \"States\", \"Usage\"])\n",
        "    input_tensor = torch.tensor(features.values, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = model(input_tensor)\n",
        "        prediction = prediction.cpu().numpy()[0][0]\n",
        "\n",
        "    actual = true_data.iloc[end_idx]['Usage']\n",
        "\n",
        "    return prediction, actual\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-6jEn84Ik_B"
      },
      "outputs": [],
      "source": [
        "prediction, actual = predict(\"Maharashtra\", \"06/01/2019\")\n",
        "print(f\"Predicted Usage: {prediction}\")\n",
        "print(f\"Actual Usage: {actual}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}