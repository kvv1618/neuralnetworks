{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDcZnwxA70sV",
        "outputId": "034641f5-6644-4ddd-f0b5-fb29bdc9d2df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ0FNvRsdpSY",
        "outputId": "5c65798c-cbb9-43c9-9dd2-d7a0e97cca78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ZBe25j9kvv"
      },
      "source": [
        "### Preprocess Climate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "3GM6HlYq9D_B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/NNProject/climate_data.csv'\n",
        "\n",
        "climate_df = pd.read_csv(file_path)\n",
        "climate_df['date'] = pd.to_datetime(climate_df['date'])\n",
        "climate_df.index = climate_df['date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "L6xgiDvl9Xzg"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "numeric_cols = climate_df.select_dtypes(include=['float64', 'int64']).columns.to_list()\n",
        "numeric_cols.remove('lat')\n",
        "numeric_cols.remove('lon')\n",
        "non_numeric_cols = climate_df.select_dtypes(exclude=['float64', 'int64']).columns.to_list() + ['lat', 'lon']\n",
        "\n",
        "climate_df[numeric_cols] = climate_df[numeric_cols].astype(float)\n",
        "climate_df[non_numeric_cols] = climate_df[non_numeric_cols]\n",
        "scaler = MinMaxScaler()\n",
        "scaled_df = pd.DataFrame(scaler.fit_transform(climate_df[numeric_cols]), columns=numeric_cols, index=climate_df.index)\n",
        "\n",
        "climate_data = pd.concat([scaled_df, climate_df[non_numeric_cols]], axis=1).reset_index(drop= True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag-kXvFbxJJU",
        "outputId": "099266fb-5277-4ed3-a665-2ed503380ab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10. 18. 14. 22.]\n",
            "[72. 80. 76. 84.]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(climate_data[\"lat\"].unique())\n",
        "print(climate_data[\"lon\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZJ0sZAq9ogi"
      },
      "source": [
        "### Preprocess Energy Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "hR1D62-Q9aFv"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/NNProject/long_data_.csv'\n",
        "\n",
        "energy_df = pd.read_csv(file_path)\n",
        "energy_df['Dates'] = pd.to_datetime(energy_df['Dates'], format='%d/%m/%Y %H:%M:%S')\n",
        "energy_df.index = energy_df['Dates']\n",
        "\n",
        "redundant_cols = [\"Regions\"]\n",
        "energy_df = energy_df.drop(columns=redundant_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "l3ng10uf9-W2"
      },
      "outputs": [],
      "source": [
        "\n",
        "numeric_cols = energy_df.select_dtypes(include=['float64', 'int64']).columns.to_list()\n",
        "numeric_cols.remove('latitude')\n",
        "numeric_cols.remove('longitude')\n",
        "non_numeric_cols = energy_df.select_dtypes(exclude=['float64', 'int64']).columns.to_list() + ['latitude', 'longitude']\n",
        "energy_df[numeric_cols] = energy_df[numeric_cols].astype(float)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_df = pd.DataFrame(scaler.fit_transform(energy_df[numeric_cols]), columns=numeric_cols, index=energy_df.index)\n",
        "energy_data = pd.concat([scaled_df, energy_df[non_numeric_cols]], axis=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-QsWQtI-BOT"
      },
      "source": [
        "### State-Lat-Lon Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NBhs-LM9_xa"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "lat_range = np.arange(10, 36, 4)\n",
        "lon_range = np.arange(72, 92, 4)\n",
        "\n",
        "energy_data[\"latitude\"] = energy_data[\"latitude\"].apply(lambda x: lat_range[np.argmin(np.abs(lat_range - x))])\n",
        "energy_data[\"longitude\"] = energy_data[\"longitude\"].apply(lambda x: lon_range[np.argmin(np.abs(lon_range - x))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySpng8nb-Jwx"
      },
      "source": [
        "## PreTrain on Climate Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtWOdSrW-MjK"
      },
      "source": [
        "### Time-Series Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "avktSMlx-ElW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def lagged_climate_input(data, seq_length=28):\n",
        "    X = []\n",
        "    for _, group in data.groupby([\"lat\", \"lon\"]):\n",
        "        group = group.sort_values(\"date\")\n",
        "        features = group.drop(columns=[\"date\", \"lat\", \"lon\"])\n",
        "        for row in range(len(features) - seq_length):\n",
        "            X.append(features.iloc[row:row+seq_length].values)\n",
        "    return np.array(X)\n",
        "\n",
        "climate_X = lagged_climate_input(climate_data, 14)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQsZGVDR-RuL"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "VpBYT8lv-OeR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ClimatePreTrainer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, latent_size=32, num_layers=1, dropout=0.3):\n",
        "        super(ClimatePreTrainer, self).__init__()\n",
        "\n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
        "                               batch_first=True, bidirectional=True, dropout=dropout)\n",
        "\n",
        "        self.to_latent = nn.Linear(hidden_size * 2, latent_size)\n",
        "        self.from_latent = nn.Linear(latent_size, hidden_size * 2)\n",
        "\n",
        "        self.decoder = nn.LSTM(hidden_size * 2, hidden_size, num_layers=num_layers,\n",
        "                               batch_first=True, bidirectional=True, dropout=dropout)\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_size * 2, input_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded_x, _ = self.encoder(x)\n",
        "        encoded_x = self.dropout(encoded_x)\n",
        "\n",
        "        latent = self.to_latent(encoded_x)\n",
        "        expanded = self.from_latent(latent)\n",
        "\n",
        "        decoded_x, _ = self.decoder(expanded)\n",
        "        decoded_x = self.dropout(decoded_x)\n",
        "\n",
        "        reconstructed_x = self.output_layer(decoded_x)\n",
        "        return reconstructed_x, latent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_EQnH0t-Ue5",
        "outputId": "040b230d-00ff-4480-fc60-ccd1f9319c34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Train Loss: 0.004214, Val Loss: 0.000861\n",
            "Validation loss improved. Saving model.\n",
            "Epoch [2/50], Train Loss: 0.001405, Val Loss: 0.000699\n",
            "Validation loss improved. Saving model.\n",
            "Epoch [3/50], Train Loss: 0.001437, Val Loss: 0.001471\n",
            "⚠️ Validation loss did not improve. Patience Counter: 1/5\n",
            "Epoch [4/50], Train Loss: 0.001482, Val Loss: 0.000790\n",
            "⚠️ Validation loss did not improve. Patience Counter: 2/5\n",
            "Epoch [5/50], Train Loss: 0.001454, Val Loss: 0.000925\n",
            "⚠️ Validation loss did not improve. Patience Counter: 3/5\n",
            "Epoch [6/50], Train Loss: 0.001453, Val Loss: 0.000784\n",
            "⚠️ Validation loss did not improve. Patience Counter: 4/5\n",
            "Epoch [7/50], Train Loss: 0.002292, Val Loss: 0.001192\n",
            "⚠️ Validation loss did not improve. Patience Counter: 5/5\n",
            "Early stopping triggered.\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchinfo import summary\n",
        "\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "lr = 0.01\n",
        "\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "climate_X_tensor = torch.tensor(climate_X, dtype=torch.float32)\n",
        "\n",
        "train_X, val_X = train_test_split(climate_X_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = TensorDataset(train_X)\n",
        "val_dataset = TensorDataset(val_X)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "climate_pretrained_model = ClimatePreTrainer(input_size=climate_X.shape[2], num_layers=2).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(climate_pretrained_model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    climate_pretrained_model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch[0].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        reconstructed_x, _ = climate_pretrained_model(batch)\n",
        "        loss = criterion(reconstructed_x, batch)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    climate_pretrained_model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch[0].to(device)\n",
        "            reconstructed_x, _ = climate_pretrained_model(batch)\n",
        "            loss = criterion(reconstructed_x, batch)\n",
        "            val_loss += loss.item()\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "        torch.save(climate_pretrained_model.state_dict(), 'climate_pretrained_model.pth')\n",
        "        print(f\"Validation loss improved. Saving model.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"Validation loss did not improve. Patience Counter: {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c99xOTMV-X4s"
      },
      "source": [
        "#### Freeze Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "IfZYqwOz-ZHH"
      },
      "outputs": [],
      "source": [
        "climate_pretrained_model = ClimatePreTrainer(input_size=climate_X.shape[2], num_layers=2).to(device)\n",
        "climate_pretrained_model.load_state_dict(torch.load('climate_pretrained_model.pth'))\n",
        "\n",
        "for param in climate_pretrained_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcasE6Y3-cLj"
      },
      "source": [
        "## FineTune Downstream"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL6KBid4-eUY"
      },
      "source": [
        "### Merge DataSources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "Ke45amvy-hwS"
      },
      "outputs": [],
      "source": [
        "energy_data = energy_data.rename(columns={\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
        "\n",
        "merged_df = pd.merge(\n",
        "    energy_data,\n",
        "    climate_data,\n",
        "    how=\"left\",\n",
        "    left_on=[\"lat\", \"lon\", \"Dates\"],\n",
        "    right_on=[\"lat\", \"lon\", \"date\"],\n",
        "    suffixes=(\"\", \"_climate\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwx6BvSaw4g2",
        "outputId": "160b8114-1bc4-4334-a789-85af86ade0fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16599, 17)"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdGFeDWkwsJH",
        "outputId": "97e37b0f-337a-442c-8db3-a04d9a70baee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5533, 17)"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df = merged_df.dropna()\n",
        "merged_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "2wdvMEoz-jZ2"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "categorical_cols = merged_df.select_dtypes(include=[\"object\"]).columns\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    merged_df[col] = le.fit_transform(merged_df[col])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "aNJkK5Eq-kpL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def lagged_merged_data(data, seq_length=28):\n",
        "    X = []\n",
        "    y = []\n",
        "    for _, group in data.groupby([\"lat\", \"lon\"]):\n",
        "        group = group.sort_values(\"date\")\n",
        "        features = group.drop(columns=[\"date\", \"Dates\", \"lat\", \"lon\", \"States\", \"Usage\"])\n",
        "        target = group[\"Usage\"]\n",
        "        for row in range(len(features) - seq_length):\n",
        "            X.append(features.iloc[row:row+seq_length].values)\n",
        "            y.append(target.iloc[row+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "merged_X, targt_y = lagged_merged_data(merged_df, 14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "C_WFj4NT-lv1"
      },
      "outputs": [],
      "source": [
        "\n",
        "class EnergyPrediction(nn.Module):\n",
        "    def __init__(self, encoder, input_size, hidden_size=64):\n",
        "        super(EnergyPrediction, self).__init__()\n",
        "        self.encoder = encoder.encoder\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded_x, _ = self.encoder(x)  # [batch, seq_len, hidden*2]\n",
        "        x = self.regressor(encoded_x[:, -1, :])  # Use last timestep\n",
        "        return x  # [batch, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "iMCBXtUp6wzj"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(merged_X, targt_y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndXMLEp9-myu",
        "outputId": "c2af1174-da93-483f-b1ee-e689d1926b14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Summary:\n",
            "EnergyPrediction(\n",
            "  (encoder): LSTM(11, 64, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (regressor): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "EnergyPrediction                         [1, 1]                    --\n",
            "├─LSTM: 1-1                              [1, 14, 128]              (138,752)\n",
            "├─Sequential: 1-2                        [1, 1]                    --\n",
            "│    └─Linear: 2-1                       [1, 64]                   8,256\n",
            "│    └─ReLU: 2-2                         [1, 64]                   --\n",
            "│    └─Linear: 2-3                       [1, 1]                    65\n",
            "==========================================================================================\n",
            "Total params: 147,073\n",
            "Trainable params: 8,321\n",
            "Non-trainable params: 138,752\n",
            "Total mult-adds (Units.MEGABYTES): 1.95\n",
            "==========================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 0.59\n",
            "Estimated Total Size (MB): 0.60\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchinfo import summary\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "climate_pretrained_model.eval()\n",
        "energy_model = EnergyPrediction(climate_pretrained_model, input_size=merged_X.shape[2]).to(device)\n",
        "print(\"Model Summary:\")\n",
        "print(energy_model)\n",
        "print(summary(energy_model, input_size=(1, 14, merged_X.shape[2])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcwzJrdKhxWq",
        "outputId": "5664e664-e7c6-4f24-d919-a99c71377e0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Loss: 0.0690\n",
            "Epoch [2/50], Loss: 0.0241\n",
            "Epoch [3/50], Loss: 0.0779\n",
            "Epoch [4/50], Loss: 0.0300\n",
            "Epoch [5/50], Loss: 0.0552\n",
            "Epoch [6/50], Loss: 0.0504\n",
            "Epoch [7/50], Loss: 0.0686\n",
            "Epoch [8/50], Loss: 0.0587\n",
            "Epoch [9/50], Loss: 0.0656\n",
            "Epoch [10/50], Loss: 0.0791\n",
            "Epoch [11/50], Loss: 0.0949\n",
            "Epoch [12/50], Loss: 0.0170\n",
            "Epoch [13/50], Loss: 0.0253\n",
            "Epoch [14/50], Loss: 0.0706\n",
            "Epoch [15/50], Loss: 0.0302\n",
            "Epoch [16/50], Loss: 0.0508\n",
            "Epoch [17/50], Loss: 0.0387\n",
            "Epoch [18/50], Loss: 0.0300\n",
            "Epoch [19/50], Loss: 0.0560\n",
            "Epoch [20/50], Loss: 0.0685\n",
            "Epoch [21/50], Loss: 0.0401\n",
            "Epoch [22/50], Loss: 0.0192\n",
            "Epoch [23/50], Loss: 0.0237\n",
            "Epoch [24/50], Loss: 0.0438\n",
            "Epoch [25/50], Loss: 0.1081\n",
            "Epoch [26/50], Loss: 0.0737\n",
            "Epoch [27/50], Loss: 0.1195\n",
            "Epoch [28/50], Loss: 0.0219\n",
            "Epoch [29/50], Loss: 0.0415\n",
            "Epoch [30/50], Loss: 0.1194\n",
            "Epoch [31/50], Loss: 0.0119\n",
            "Epoch [32/50], Loss: 0.0372\n",
            "Epoch [33/50], Loss: 0.0591\n",
            "Epoch [34/50], Loss: 0.0194\n",
            "Epoch [35/50], Loss: 0.0632\n",
            "Epoch [36/50], Loss: 0.0405\n",
            "Epoch [37/50], Loss: 0.0331\n",
            "Epoch [38/50], Loss: 0.0294\n",
            "Epoch [39/50], Loss: 0.0667\n",
            "Epoch [40/50], Loss: 0.0425\n",
            "Epoch [41/50], Loss: 0.0760\n",
            "Epoch [42/50], Loss: 0.0468\n",
            "Epoch [43/50], Loss: 0.0511\n",
            "Epoch [44/50], Loss: 0.0888\n",
            "Epoch [45/50], Loss: 0.0462\n",
            "Epoch [46/50], Loss: 0.0492\n",
            "Epoch [47/50], Loss: 0.0403\n",
            "Epoch [48/50], Loss: 0.0168\n",
            "Epoch [49/50], Loss: 0.1115\n",
            "Epoch [50/50], Loss: 0.0465\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "num_epochs = 50\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(energy_model.parameters(), lr=lr)\n",
        "energy_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        inputs, targets = batch[0].to(device), batch[1].to(device)\n",
        "        targets = targets.view(-1, 1)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = energy_model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6Xyv7rb-nza",
        "outputId": "d6a89269-ee50-4282-b2eb-e5af8d4376d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Summary:\n",
            "EnergyPrediction(\n",
            "  (encoder): LSTM(11, 64, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (regressor): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "EnergyPrediction                         [1, 1]                    --\n",
            "├─LSTM: 1-1                              [1, 14, 128]              138,752\n",
            "├─Sequential: 1-2                        [1, 1]                    --\n",
            "│    └─Linear: 2-1                       [1, 64]                   8,256\n",
            "│    └─ReLU: 2-2                         [1, 64]                   --\n",
            "│    └─Linear: 2-3                       [1, 1]                    65\n",
            "==========================================================================================\n",
            "Total params: 147,073\n",
            "Trainable params: 147,073\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.MEGABYTES): 1.95\n",
            "==========================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 0.59\n",
            "Estimated Total Size (MB): 0.60\n",
            "==========================================================================================\n",
            "Epoch [1/50], Loss: 0.0583\n",
            "Epoch [2/50], Loss: 0.0158\n",
            "Epoch [3/50], Loss: 0.0333\n",
            "Epoch [4/50], Loss: 0.0213\n",
            "Epoch [5/50], Loss: 0.0164\n",
            "Epoch [6/50], Loss: 0.0090\n",
            "Epoch [7/50], Loss: 0.1790\n",
            "Epoch [8/50], Loss: 0.0168\n",
            "Epoch [9/50], Loss: 0.0806\n",
            "Epoch [10/50], Loss: 0.0298\n",
            "Epoch [11/50], Loss: 0.0131\n",
            "Epoch [12/50], Loss: 0.0030\n",
            "Epoch [13/50], Loss: 0.1498\n",
            "Epoch [14/50], Loss: 0.1359\n",
            "Epoch [15/50], Loss: 0.1401\n",
            "Epoch [16/50], Loss: 0.0296\n",
            "Epoch [17/50], Loss: 0.0039\n",
            "Epoch [18/50], Loss: 0.0396\n",
            "Epoch [19/50], Loss: 0.0384\n",
            "Epoch [20/50], Loss: 0.0274\n",
            "Epoch [21/50], Loss: 0.0520\n",
            "Epoch [22/50], Loss: 0.0216\n",
            "Epoch [23/50], Loss: 0.0593\n",
            "Epoch [24/50], Loss: 0.0266\n",
            "Epoch [25/50], Loss: 0.0930\n",
            "Epoch [26/50], Loss: 0.1137\n",
            "Epoch [27/50], Loss: 0.0077\n",
            "Epoch [28/50], Loss: 0.0180\n",
            "Epoch [29/50], Loss: 0.0589\n",
            "Epoch [30/50], Loss: 0.0188\n",
            "Epoch [31/50], Loss: 0.0548\n",
            "Epoch [32/50], Loss: 0.0076\n",
            "Epoch [33/50], Loss: 0.0029\n",
            "Epoch [34/50], Loss: 0.0084\n",
            "Epoch [35/50], Loss: 0.0261\n",
            "Epoch [36/50], Loss: 0.0501\n",
            "Epoch [37/50], Loss: 0.0397\n",
            "Epoch [38/50], Loss: 0.0188\n",
            "Epoch [39/50], Loss: 0.0374\n",
            "Epoch [40/50], Loss: 0.0051\n",
            "Epoch [41/50], Loss: 0.0380\n",
            "Epoch [42/50], Loss: 0.0443\n",
            "Epoch [43/50], Loss: 0.0185\n",
            "Epoch [44/50], Loss: 0.1242\n",
            "Epoch [45/50], Loss: 0.0295\n",
            "Epoch [46/50], Loss: 0.0294\n",
            "Epoch [47/50], Loss: 0.0307\n",
            "Epoch [48/50], Loss: 0.0215\n",
            "Epoch [49/50], Loss: 0.0890\n",
            "Epoch [50/50], Loss: 0.0474\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for param in energy_model.encoder.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(\"Model Summary:\")\n",
        "print(energy_model)\n",
        "print(summary(energy_model, input_size=(1, 14, merged_X.shape[2])))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        inputs, targets = batch[0].to(device), batch[1].to(device)\n",
        "        targets = targets.view(-1, 1)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = energy_model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pyx4avV-pa8",
        "outputId": "45269dec-ecd1-4c8b-92a1-a8830cd58731"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.0356\n"
          ]
        }
      ],
      "source": [
        "energy_model.eval()\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "val_loss = 0\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        inputs, targets = batch[0].to(device), batch[1].to(device)\n",
        "        targets = targets.view(-1, 1)\n",
        "        outputs = energy_model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        val_loss += loss.item()\n",
        "avg_val_loss = val_loss / len(val_loader)\n",
        "print(f\"Validation Loss: {avg_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMv43iZQ7S4w"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
